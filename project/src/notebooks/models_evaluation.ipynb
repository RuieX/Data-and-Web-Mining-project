{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "We have done feature engineering on the raw dataset and got our final dataset.\n",
    "Now we will use three different models for predictions:\n",
    "- Linear Regression\n",
    "- Decision Tree\n",
    "- Random Forest Regression\n",
    "- Regression Enhanced Random Forest\n",
    "\n",
    "For each model, predictions are made, after a training and parameter tuning phase, on each of the datasets produced in the feat-engineering notebook.\n",
    "Results for each set of predictions are then plotted and visualized."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Multiple Models\n",
    "\n",
    "Now that we've tested our data preparation pipeline with a sample model, the next step is to train the data on different regression algorithms to shortlist the most promising algorithms for our problem.\n",
    "\n",
    "Algorithms to test with include:\n",
    "- **Linear Regression**: Simple algorithm to implement but can over-simplify real-world problems by assuming a linear relationship among the variables.\n",
    "- **Support Vector Regression**: Uses hyperplanes to segregate the data.\n",
    "- **Decision Tree**: Powerful model capable of finding complex nonlinear relationships in the data.\n",
    "- **Random Forest**: Train many Decision Tress on random subsets of the features (*Ensemble Learning*).\n",
    "\n",
    "- i.Adaboost Regressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "Let us import the required modules."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "from math import sqrt\n",
    "import pickle\n",
    "\n",
    "import project.src.feat_eng as fe\n",
    "import project.src.visualization as viz\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "%matplotlib inline\n",
    "sys.path.insert(0, os.path.abspath(\"../../\"))\n",
    "color = sns.color_palette()\n",
    "pd.set_option(\"display.max_columns\", 100) #\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "Note that the dataset is already split into Train-Test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "engineered_dataset = fe.TrainTestSplit.from_csv_directory(dir_path=\"../data/lvl4_rfecv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62090 entries, 0 to 62089\n",
      "Data columns (total 23 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   bathroomcnt                   62090 non-null  float64\n",
      " 1   bedroomcnt                    62090 non-null  float64\n",
      " 2   fireplacecnt                  62090 non-null  float64\n",
      " 3   garagecarcnt                  62090 non-null  float64\n",
      " 4   latitude                      62090 non-null  float64\n",
      " 5   longitude                     62090 non-null  float64\n",
      " 6   poolcnt                       62090 non-null  float64\n",
      " 7   roomcnt                       62090 non-null  float64\n",
      " 8   threequarterbathnbr           62090 non-null  float64\n",
      " 9   unitcnt                       62090 non-null  float64\n",
      " 10  numberofstories               62090 non-null  float64\n",
      " 11  house_age                     62090 non-null  float64\n",
      " 12  heatingorsystemtypeid_2.0     62090 non-null  float64\n",
      " 13  heatingorsystemtypeid_7.0     62090 non-null  float64\n",
      " 14  propertylandusetypeid_261.0   62090 non-null  float64\n",
      " 15  propertylandusetypeid_266.0   62090 non-null  float64\n",
      " 16  taxdelinquencyflag_Y          62090 non-null  float64\n",
      " 17  calculatedfinishedsquarefeet  62090 non-null  float64\n",
      " 18  garagetotalsqft               62090 non-null  float64\n",
      " 19  lotsizesquarefeet             62090 non-null  float64\n",
      " 20  landtaxvaluedollarcnt         62090 non-null  float64\n",
      " 21  structuretaxvaluedollarcnt    62090 non-null  float64\n",
      " 22  yardbuildingsqft17            62090 non-null  float64\n",
      "dtypes: float64(23)\n",
      "memory usage: 10.9 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62090 entries, 0 to 62089\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       62090 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 485.2 KB\n"
     ]
    }
   ],
   "source": [
    "engineered_dataset.x_train.info()\n",
    "engineered_dataset.y_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------ lotto -------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------fine lotto ---------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------------- inizio pier ---------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hyper parameters #devo inventarmi una scusa e fare qualcosa o uno screen, chiedere a pier per sta roba"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is already a selection after many grid-search runs\n",
    "RF_HYPER_PARAMS = {\n",
    "    \"n_estimators\": [150, 200, 250],\n",
    "    \"min_samples_leaf\": [1, 50, 100, 200],\n",
    "    \"max_leaf_nodes\": [2, 5, 10],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@utils.print_time_perf\n",
    "def evaluate_lin_reg(datasets: dict[str, tr.TrainTestSplit]) \\\n",
    "        -> (list[ev.RegressorEvaluation], list[ev.RegressorEvaluation]):\n",
    "\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for name, data in datasets.items():\n",
    "        print(f\"---- Now using {name} dataset ----\")\n",
    "        reg = sup.linear_regressor_fit(data.x_train.values, data.y_train, n_jobs=N_JOBS)\n",
    "\n",
    "        train_eval, test_eval = ev.evaluate_performance(reg, name, data)\n",
    "        train_results.append(train_eval)\n",
    "        test_results.append(test_eval)\n",
    "\n",
    "    return train_results, test_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"------ LINEAR REGRESSION ------\\n\")\n",
    "lin_reg_train_results, lin_reg_test_results = evaluate_lin_reg(engineered_datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save progress so that training doesn't have to be run again\n",
    "dm.store_evaluations(dir_path=LIN_REG_DIR, evals=lin_reg_test_results)\n",
    "\n",
    "train_dir = f\"{LIN_REG_DIR}/train\"\n",
    "dm.create_dirs_if_not_exists(dir_paths=[train_dir])\n",
    "dm.store_evaluations(dir_path=train_dir, evals=lin_reg_train_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "ev.print_evaluation_stats(lin_reg_train_results)\n",
    "\n",
    "print(\"Testing:\")\n",
    "ev.print_evaluation_stats(lin_reg_test_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@utils.print_time_perf\n",
    "def evaluate_rf_reg(datasets: dict[str, tr.TrainTestSplit])\n",
    "        -> (list[ev.RegressorEvaluation], list[ev.RegressorEvaluation]):\n",
    "\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for name, data in datasets.items():\n",
    "        print(f\"---- Now using {name} dataset ----\")\n",
    "\n",
    "        print(\"Tuning...\")\n",
    "        rf_reg = RandomForestRegressor(n_jobs=N_JOBS, random_state=RND_SEED)\n",
    "        search_result = tr.grid_search_cv_tuning(model=rf_reg,\n",
    "                                                 train_data=data.x_train.values,\n",
    "                                                 train_target=data.y_train,\n",
    "                                                 hyper_params=RF_HYPER_PARAMS,\n",
    "                                                 scoring=\"neg_mean_squared_error\",\n",
    "                                                 k_folds=4, n_jobs=N_JOBS, verbosity=VERBOSITY)\n",
    "        print(\"Tuning results:\")\n",
    "        print(f\"Best params: {search_result.best_params_}\")\n",
    "\n",
    "        print(\"Fitting with best params and full training set...\")\n",
    "        tuned_rf = RandomForestRegressor(n_jobs=N_JOBS, random_state=RND_SEED,\n",
    "                                         **search_result.best_params_)\n",
    "        tuned_rf.fit(X=data.x_train.values, y=data.y_train)\n",
    "\n",
    "        print(\"Evaluating performance...\")\n",
    "        train_eval, test_eval = ev.evaluate_performance(tuned_rf, name, data)\n",
    "        train_results.append(train_eval)\n",
    "        test_results.append(test_eval)\n",
    "\n",
    "    return train_results, test_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"------- RF REGRESSION -------\\n\")\n",
    "rf_reg_train_results, rf_reg_test_results = evaluate_rf_reg(engineered_datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save progress so that training doesn't have to be run again\n",
    "dm.store_evaluations(dir_path=RF_REG_DIR, evals=rf_reg_test_results)\n",
    "\n",
    "train_dir = f\"{RF_REG_DIR}/train\"\n",
    "dm.create_dirs_if_not_exists(dir_paths=[train_dir])\n",
    "dm.store_evaluations(dir_path=train_dir, evals=rf_reg_train_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "ev.print_evaluation_stats(rf_reg_train_results)\n",
    "\n",
    "print(\"Testing:\")\n",
    "ev.print_evaluation_stats(rf_reg_test_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "qua ha fatto regression enhanced random forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performance Visualization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Order results by regressor_id (dataset name) so that plots are ordered\n",
    "key_selector = lambda x: x.regressor_id\n",
    "\n",
    "lin_reg_train_results = list(sorted(dm.load_evaluations(dir_path=f\"{LIN_REG_DIR}/train\"), key=key_selector))\n",
    "rf_reg_train_results = list(sorted(dm.load_evaluations(dir_path=f\"{RF_REG_DIR}/train\"), key=key_selector))\n",
    "refr_train_results = list(sorted(dm.load_evaluations(dir_path=f\"{REFR_DIR}/train\"), key=key_selector))\n",
    "\n",
    "lin_reg_test_results = list(sorted(dm.load_evaluations(dir_path=LIN_REG_DIR), key=key_selector))\n",
    "rf_reg_test_results = list(sorted(dm.load_evaluations(dir_path=RF_REG_DIR), key=key_selector))\n",
    "refr_test_results = list(sorted(dm.load_evaluations(dir_path=REFR_DIR), key=key_selector))\n",
    "\n",
    "training_results = {\n",
    "    \"[Training] Linear Regression\": lin_reg_train_results,\n",
    "    \"[Training] Random Forest Regression\": rf_reg_train_results,\n",
    "    \"[Training] Regression Enhanced Random Forest\": refr_train_results\n",
    "}\n",
    "\n",
    "testing_results = {\n",
    "    \"Linear Regression\": lin_reg_test_results,\n",
    "    \"Random Forest Regression\": rf_reg_test_results,\n",
    "    \"Regression Enhanced Random Forest\": refr_test_results\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_performance_df(results: dict[str, list[ev.RegressorEvaluation]]):\n",
    "    perf_records = []\n",
    "    for model_name, evaluations in results.items():\n",
    "        for evl in evaluations:\n",
    "            record = {\n",
    "                \"model\": model_name,\n",
    "                \"dataset id\": evl.regressor_id,\n",
    "                \"MAE\": evl.mae,\n",
    "                \"MSE\": evl.mse,\n",
    "                \"R2\": evl.r2\n",
    "            }\n",
    "            perf_records.append(record)\n",
    "\n",
    "    return pd.DataFrame.from_records(data=perf_records).sort_values(by=\"dataset id\")\n",
    "\n",
    "def performance_plot(performance_df: pd.DataFrame):\n",
    "    plot = sns.lineplot(data=performance_df, x=\"dataset id\", y=\"MSE\", hue=\"model\",\n",
    "                        style=\"model\", palette=\"pastel\", markers=True)\n",
    "    plot.tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "    return plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_features_vs_predictions(evaluation: ev.RegressorEvaluation):\n",
    "    dataset_name = evaluation.regressor_id\n",
    "    dataset = engineered_datasets[dataset_name]\n",
    "    test_data = dataset.x_test\n",
    "\n",
    "    fig, axs = vis.bivariate_feature_plot(data=test_data, mode=\"scatter\",\n",
    "                                          y_var=(\"Model predictions\", pd.Series(evaluation.y_pred)),\n",
    "                                          subplot_size=(5, 4),\n",
    "                                          width=3, title_size=50,\n",
    "                                          title=f\"[{dataset_name}] Features vs Predictions\",\n",
    "                                          scatter_kwargs={\n",
    "                                              \"alpha\": 0.8\n",
    "                                          })\n",
    "\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "def plot_features_vs_residuals(evaluation: ev.RegressorEvaluation):\n",
    "    dataset_name = evaluation.regressor_id\n",
    "    dataset = engineered_datasets[dataset_name]\n",
    "    test_data = dataset.x_test\n",
    "\n",
    "    residuals = evaluation.y_true - evaluation.y_pred\n",
    "    fig, axs = vis.bivariate_feature_plot(data=test_data, mode=\"scatter\",\n",
    "                                          y_var=(\"Model residuals\", pd.Series(residuals)),\n",
    "                                          subplot_size=(5, 4),\n",
    "                                          width=3, title_size=50,\n",
    "                                          title=f\"[{dataset_name}] Features vs Residuals\",\n",
    "                                          scatter_kwargs={\n",
    "                                              \"alpha\": 0.8\n",
    "                                          })\n",
    "\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "def get_extreme_predictions(data: pd.DataFrame, evaluation: ev.RegressorEvaluation, percentile: float):\n",
    "    most_wrong = ev.get_highest_error_instances(data=data, percentile=percentile,\n",
    "                                                pred=evaluation.y_pred, true_pred=evaluation.y_true,\n",
    "                                                error_type=\"squared\")\n",
    "    most_correct = ev.get_lowest_error_instances(data=data, percentile=percentile,\n",
    "                                                 pred=evaluation.y_pred, true_pred=evaluation.y_true,\n",
    "                                                 error_type=\"squared\")\n",
    "\n",
    "    return most_wrong, most_correct\n",
    "\n",
    "\n",
    "color_red = \"#bf1515\"\n",
    "color_green = \"#32a852\"\n",
    "\n",
    "\n",
    "def plot_extreme_instances_on_distribution(evaluation: ev.RegressorEvaluation):\n",
    "    dataset_name = evaluation.regressor_id\n",
    "    dataset = engineered_datasets[dataset_name]\n",
    "    test_data = dataset.x_test.copy()\n",
    "\n",
    "    fig, axs = vis.feature_distributions_plot(data=test_data, numerical_mode=\"violin\",\n",
    "                                              subplot_size=(5, 4),\n",
    "                                              width=5, title_size=40,\n",
    "                                              title=f\"[{dataset_name}] Most Wrong/Correct on Distributions\")\n",
    "\n",
    "    most_wrong, most_correct = get_extreme_predictions(data=test_data,\n",
    "                                                       evaluation=evaluation,\n",
    "                                                       percentile=99.5)\n",
    "    for ax in axs.flatten():\n",
    "        feature_name = ax.get_xlabel()\n",
    "        if feature_name != \"\":\n",
    "            x_worst = most_wrong[feature_name].values\n",
    "            x_best = most_correct[feature_name].values\n",
    "\n",
    "            # Points are drawn at mid height + an offset so they don't overlap\n",
    "            y_min, y_max = ax.get_ylim()\n",
    "            height = (abs(y_max) - abs(y_min))\n",
    "            half_height = height / 2\n",
    "\n",
    "            # Heights of worst and best on two different levels\n",
    "            y_worst_height = half_height + 1\n",
    "            y_best_height = half_height + 2\n",
    "\n",
    "            # Also add gaussian noise to mitigate overlapping with the violin/box plot\n",
    "            y_worst = [y_worst_height + np.random.normal(0, 0.05) for _ in range(len(x_worst))]\n",
    "            y_best = [y_best_height + np.random.normal(0, 0.05) for _ in range(len(x_best))]\n",
    "\n",
    "            # Plot the most wrong/correct values over the distribution plots\n",
    "            # and assign them size in proportion to their wrongness/correctness\n",
    "            worst_size = 50 * ((np.argsort(most_wrong[\"errors\"].values) + 1) / len(x_worst))\n",
    "            best_size = 50 * ((np.argsort(-most_correct[\"errors\"].values) + 1) / len(x_best))\n",
    "\n",
    "            ax.scatter(x=x_worst, y=y_worst, s=worst_size, c=color_red)\n",
    "            ax.scatter(x=x_best, y=y_best, s=best_size, c=color_green)\n",
    "\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "def plot_extreme_instances_on_feature_vs_target(evaluation: ev.RegressorEvaluation):\n",
    "    dataset_name = evaluation.regressor_id\n",
    "    dataset = engineered_datasets[dataset_name]\n",
    "    test_data = dataset.x_test\n",
    "\n",
    "    fig, axs = vis.bivariate_feature_plot(data=test_data, mode=\"scatter\",\n",
    "                                          y_var=(\"True logerror\", pd.Series(dataset.y_test)),\n",
    "                                          subplot_size=(5, 4),\n",
    "                                          width=3, title_size=40,\n",
    "                                          title=f\"[{dataset_name}] Most Wrong/Correct on Features vs Target\",\n",
    "                                          scatter_kwargs={\n",
    "                                              \"alpha\": 0.65  # so that extreme instances are highlighted\n",
    "                                          })\n",
    "\n",
    "    most_wrong, most_correct = get_extreme_predictions(data=test_data,\n",
    "                                                       evaluation=evaluation,\n",
    "                                                       percentile=99.5)\n",
    "    for ax in axs.flatten():\n",
    "        feature_name = ax.get_xlabel()\n",
    "        if feature_name != \"\":\n",
    "            x_worst = most_wrong[feature_name].values\n",
    "            x_best = most_correct[feature_name].values\n",
    "\n",
    "            y_worst = most_wrong[\"true predictions\"].values\n",
    "            y_best = most_correct[\"true predictions\"].values\n",
    "\n",
    "            # Plot the most wrong/correct values over the distribution plots\n",
    "            # and assign them size in proportion to their wrongness/correctness\n",
    "            worst_size = 80 * ((np.argsort(most_wrong[\"errors\"].values) + 1) / len(x_worst))\n",
    "            best_size = 80 * ((np.argsort(-most_correct[\"errors\"].values) + 1) / len(x_best))\n",
    "\n",
    "            ax.scatter(x=x_worst, y=y_worst, s=worst_size, c=color_red)\n",
    "            ax.scatter(x=x_best, y=y_best, s=best_size, c=color_green)\n",
    "\n",
    "    return fig, axs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_performance_df = get_performance_df(training_results)\n",
    "train_performance_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "performance_plot(train_performance_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_performance_df = get_performance_df(testing_results)\n",
    "test_performance_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "performance_plot(test_performance_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training vs Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_test_perf_df = pd.concat([train_performance_df, test_performance_df]).reset_index(drop=True)\n",
    "train_test_perf_df = train_test_perf_df.sort_values(by=\"dataset id\")\n",
    "train_test_perf_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "performance_plot(train_test_perf_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predictions and Residuals\n",
    "\n",
    "In this section, for each combination of model and testing set, 2 types of plots will be shown:\n",
    "- Features vs model predictions;\n",
    "- Features vs model residuals.\n",
    "\n",
    "Here the goal is to understand how the models make their predictions and get a general idea of where and how wrong they are."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for result in lin_reg_test_results:\n",
    "    plot_features_vs_predictions(evaluation=result)\n",
    "    plot_features_vs_residuals(evaluation=result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for result in rf_reg_test_results:\n",
    "    plot_features_vs_predictions(evaluation=result)\n",
    "    plot_features_vs_residuals(evaluation=result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regression Enhanced Random Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A Look at the Expected Value and Variance of True log-errors\n",
    "\n",
    "The expected value of both training and testing sets' targets is shown to provide a better context around the previously plotted predictions. We can see that the expected value is around 0.017, which is very close to what the models are predicting: in other words, the models seem to be predicting values around the average of the true log-errors with relatively little variance.\n",
    "\n",
    "A look at the variance of true log-errors also gives an idea as to why the testing sets perform much better than the training ones: since all the models seem to predict the average log-error, or very close to it, for each instance, error is expected to be directly proportional with the variance of the target of each set. The variance of the testing set is, in fact, lower than the training set one, most likely because its size is smaller."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Since all y_train and y_test are equal, the dataset from which they are extracted\n",
    "# does not matter\n",
    "any_dataset = \"lvl1-leave-one-out\"\n",
    "y_train = engineered_datasets[any_dataset].y_train\n",
    "y_test = engineered_datasets[any_dataset].y_test\n",
    "\n",
    "pd.DataFrame(data={\n",
    "    \"Set\": [\"Training\", \"Testing\"],\n",
    "    \"Expected value\": [y_train.mean(), y_test.mean()],\n",
    "    \"Variance\": [y_train.var(), y_test.var()]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Best and Worst instances\n",
    "\n",
    "In this section, for each combination of model and testing set, 2 types of plots will be shown:\n",
    "- Distribution of extreme instances (in terms of predictions) vs actual feature distribution;\n",
    "- Dataset features and extreme instances vs true logerror.\n",
    "\n",
    "Both plots' goal is to help me understand if there is some peculiarity in the distribution and predictions of the most wrongly/correctly predicted instances."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for result in lin_reg_test_results:\n",
    "    plot_extreme_instances_on_distribution(evaluation=result)\n",
    "    plot_extreme_instances_on_feature_vs_target(evaluation=result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for result in rf_reg_test_results:\n",
    "    plot_extreme_instances_on_distribution(evaluation=result)\n",
    "    plot_extreme_instances_on_feature_vs_target(evaluation=result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------ fine pier -----------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------- DA NOTEBOOK CRASTO -----------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Baseline Metrics\n",
    "\n",
    "It is important to set a baseline for the model's performance to compare different algorithms. For regression problems, the baseline metrics are calculated by replacing $y'$ with $\\bar{y}$. Using this, the different baseline regression metrics are:\n",
    "\n",
    "- **MSE Baseline**: Variance of the target variable (Mean Squared Error)\n",
    "- **RMSE Baseline**: Standard Deviation of the target variable (Root Mean Squared Error)\n",
    "- **MAE Baseline**: Average Abolsute Deviation of the target variable (Mean Absolute Error)\n",
    "- **R2 Baseline**: 0\n",
    "\n",
    "For this regression problem, we will use the models' **Mean Absolute Error** and **RMSE (Root Mean Squared Error)** to compare the different algorithms which have **baseline values of 0.533 and 0.0837** respectively.\n",
    "\n",
    "We will also observe the RMSE as another evaluation metric which punishes more for outliers than MAE."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Baseline for RMSE\n",
    "print(f\"MAE Baseline: {engineered_dataset.y_train.mad()}\")\n",
    "print(f\"RMSE Baseline: {engineered_dataset.y_train.std()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MAE Evaluation\n",
    "\n",
    "To evaluate and short list the most promising models, we will use the models' **MAE** in two different ways:\n",
    "\n",
    "1) **MAE on Validation Set**: Calculates the MAE on the validation set which is quicker to calculate than evaluation using Cross-Validation. However, it is possible the MAE obtained is skewed depending on the instances sampled in the validation set.\n",
    "\n",
    "2) A great alternative is to use **K-Fold Cross-Validation** where the training set is randomly split into `n` subsets (for example 10 subsets) called *folds*. It trains and evaluates the model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. Result is an array containing the 10 evaluation scores. Takes longer to evaluate but provides a more accurate measure of the model's performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_eval_metrics(models, X, y_true):\n",
    "    \"\"\"\n",
    "    Calculates MAE (Mean Absoulate Error) and RMSE (Root Mean Squared Error) on the data set for input models.\n",
    "    `models`: list of fit models\n",
    "    \"\"\"\n",
    "    for model in models:\n",
    "        y_pred= model.predict(X)\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"MAE: {mae}, RMSE: {rmse}\")\n",
    "\n",
    "# Test usage of RMSE function\n",
    "# get_eval_metrics([lin_reg, ridge_reg, lasso_reg], X_prepared_val, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_scores(model, scores):\n",
    "    print(\"-\"*50)\n",
    "    print(\"Model:\", model)\n",
    "    print(\"\\nScores:\", scores)\n",
    "    print(\"\\nMean:\", scores.mean())\n",
    "    print(\"\\nStandard deviation:\", scores.std())\n",
    "\n",
    "def get_cross_val_scores(models, X, y, cv=10, fit_params=None):\n",
    "    \"\"\"\n",
    "    Performs k-fold cross validation and calculates MAE for each fold for all input models.\n",
    "    `models`: list of fit models\n",
    "    \"\"\"\n",
    "    for model in models:\n",
    "        mae = -cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\", cv=cv, fit_params=fit_params)\n",
    "        display_scores(model, mae)\n",
    "\n",
    "    # Test usage of cross val function\n",
    "# get_cross_val_scores([lin_reg, ridge_reg], X_prepared, y_train, cv=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear Regression: Plain linear regression that minimizes the Mean Squared Error(MSE) cost function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model RMSE is significantly higher than MAE which suggests that the outliers are affecting the model's performance as RMSE punishes the model more for mispredicting outliers.\n",
    "The K-Fold Cross Validation shows that the model's performance is highly volatile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegression()"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(engineered_dataset.x_train, engineered_dataset.y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 0.07225147731871365\n",
      "\n",
      "Mean Squared Error : 0.033117004168537564\n",
      "\n",
      "Root Mean Squared Error : 0.1819807796679022\n"
     ]
    }
   ],
   "source": [
    "linear_reg_pred = linear_reg.predict(engineered_dataset.x_test)\n",
    "\n",
    "print('Mean Absolute Error : {}'.format(mean_absolute_error(engineered_dataset.y_test, linear_reg_pred)))\n",
    "print()\n",
    "print('Mean Squared Error : {}'.format(mean_squared_error(engineered_dataset.y_test, linear_reg_pred)))\n",
    "print()\n",
    "print('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(engineered_dataset.y_test, linear_reg_pred))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dopo fit (notebook crasto)\n",
    "# fa osservazioni craste ma complicate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ada Boost Regression Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "AdaBoostRegressor()"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_reg = AdaBoostRegressor()\n",
    "\n",
    "adaboost_reg.fit(engineered_dataset.x_train, engineered_dataset.y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 0.20165288729791706\n",
      "\n",
      "Mean Squared Error : 0.08715703095313854\n",
      "\n",
      "Root Mean Squared Error : 0.29522369646276453\n"
     ]
    }
   ],
   "source": [
    "adaboost_reg_pred = adaboost_reg.predict(engineered_dataset.x_test)\n",
    "\n",
    "print('Mean Absolute Error : {}'.format(mean_absolute_error(engineered_dataset.y_test, adaboost_reg_pred)))\n",
    "print()\n",
    "print('Mean Squared Error : {}'.format(mean_squared_error(engineered_dataset.y_test, adaboost_reg_pred)))\n",
    "print()\n",
    "print('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(engineered_dataset.y_test, adaboost_reg_pred))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Tree Regressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decision Tree: Powerful model capable of finding complex nonlinear relationships in the data.\n",
    "Random Forest: Train many Decision Tress on random subsets of the features via the bagging method (Ensemble Learning)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeRegressor(max_depth=5)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "tree_reg.fit(engineered_dataset.x_train, engineered_dataset.y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 0.07222576974868114\n",
      "\n",
      "Mean Squared Error : 0.033611091729615135\n",
      "\n",
      "Root Mean Squared Error : 0.18333328047470032\n"
     ]
    }
   ],
   "source": [
    "tree_reg_pred = tree_reg.predict(engineered_dataset.x_test)\n",
    "\n",
    "print('Mean Absolute Error : {}'.format(mean_absolute_error(engineered_dataset.y_test, tree_reg_pred)))\n",
    "print()\n",
    "print('Mean Squared Error : {}'.format(mean_squared_error(engineered_dataset.y_test, tree_reg_pred)))\n",
    "print()\n",
    "print('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(engineered_dataset.y_test, tree_reg_pred))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Regression Model¶"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_c/vnjyd7f9581cdkdmh1wpts080000gn/T/ipykernel_77011/3605848252.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  forest_reg.fit(engineered_dataset.x_train, engineered_dataset.y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomForestRegressor(max_depth=6, n_estimators=50)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg = RandomForestRegressor(n_estimators= 50, max_depth=6)\n",
    "\n",
    "forest_reg.fit(engineered_dataset.x_train, engineered_dataset.y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 0.07201648566472933\n",
      "\n",
      "Mean Squared Error : 0.03331842958157641\n",
      "\n",
      "Root Mean Squared Error : 0.18253336566659917\n"
     ]
    }
   ],
   "source": [
    "forest_reg_pred = forest_reg.predict(engineered_dataset.x_test)\n",
    "\n",
    "print('Mean Absolute Error : {}'.format(mean_absolute_error(engineered_dataset.y_test, forest_reg_pred)))\n",
    "print()\n",
    "print('Mean Squared Error : {}'.format(mean_squared_error(engineered_dataset.y_test, forest_reg_pred)))\n",
    "print()\n",
    "print('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(engineered_dataset.y_test, forest_reg_pred))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross Validation & Hyperparameter Optimization for Random Forest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(forest_reg, engineered_dataset.x_train, engineered_dataset.y_train, scoring=\"neg_mean_squared_error\", cv = 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.16437461, 0.16956266, 0.16824223, 0.15338318, 0.17947782])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg_rmse_scores = np.sqrt(-scores)\n",
    "forest_reg_rmse_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [300, 400, 500], 'max_features': [2, 4, 6]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 6, 9], 'max_features': [2, 4, 6]}]\n",
    "\n",
    "forest_regressor = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_regressor, param_grid, scoring='neg_mean_squared_error',return_train_score=True,cv=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(engineered_dataset.x_train, engineered_dataset.y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# final_predictor = grid_search.best_estimator_\n",
    "# final_predictor.fit(engineered_dataset.x_train, engineered_dataset.y_train)\n",
    "# final_pred = final_predictor.predict(engineered_dataset.x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print('Mean Absolute Error : {}'.format(mean_absolute_error(engineered_dataset.y_test, final_pred)))\n",
    "# print()\n",
    "# print('Mean Squared Error : {}'.format(mean_squared_error(engineered_dataset.y_test, final_pred)))\n",
    "# print()\n",
    "# print('Root Mean Squared Error : {}'.format(sqrt(mean_squared_error(engineered_dataset.y_test, final_pred))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving the model\n",
    "# file_name = 'final_pickle_model.pickle'\n",
    "# pickle.dump(final_predictor,open(file_name,'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "#\n",
    "# attrs = list(engineered_dataset.select_dtypes(include = ['float64','int64']))\n",
    "#\n",
    "# sorted(zip(attrs, feature_importances), reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving Predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model_pred = pd.DataFrame({'parcelid':X_test_new.parcelid, 'logerror':final_pred})\n",
    "# model_pred.to_csv('model_predictions.csv',index=False)\n",
    "# model_pred.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "1. I have performed all the feature engineering steps necessary to ensure the dataset is ready to be fed into Machine Learning algorithms.\n",
    "\n",
    "2. After Pre-processing and Feature Engineering the raw dataset we splitted the dataset into train and test sets.\n",
    "\n",
    "3. Performed Feature scaling on data for better performance.\n",
    "\n",
    "4. Trained multiple models using different ML regression algorithms on dataset.\n",
    "\n",
    "5. Appleied Performance metrics such as MAE, MSE, RMSE to find out best prediction model.\n",
    "\n",
    "6. With the help of GridSearch CV we found out best estimator with least Root mean squred error.\n",
    "\n",
    "7. Saved best predictor in .pickle format for future predictions.\n",
    "\n",
    "8. Done prediction on test data and saved predictions into .csv file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hypermeter Tuning (GridSearchCV)\n",
    "\n",
    "a.For Random Forest Regressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking for Feature Importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the final model and making predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}